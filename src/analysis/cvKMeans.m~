function [c, mu, distortionValue, k] = cvKMeans(X, kRange, method)
% Use
%   Implementation of k-means with k-fold or hold-out cross-validation to 
%   determine the best number of clusters to use within user-input range.
% Input
%   X : m-samples training set, where each row is a sample feature vector
%   kRange : range of k-values for cross-validation
%   method : string indicating either k-fold ('kfold') or hold-out 
%            ('holdout') cross validation method
% Output
%   c : cluster labels for each point
%   mu : cluster centroid positions, where each column is a centroid
%   distortionValue: distortion function value
%   k : best k-value out of kRange

    % constants
    KFOLD = 'kfold';
    HOLDOUT = 'holdout';

    % check arguments
    if sum(kRange < 1) > 0
        error('Error: kRange must only contain positive integers');
    elseif ~strcmp(method, KFOLD) || ~strcmp(method, HOLDOUT)
        error('Error: method can only be kfold or holdout');
    end % if

    % run cross-validation
    if strcmp(method, KFOLD)
        [c, mu, distortionValue, k] = kfold(X, kRange, method);
    else % strcmp(method, HOLDOUT)
        [c, mu, distortionValue, k] = holdout(X, kRange, method);
    end % if
    
end


function [c, mu, distortionValue, k] = holdout(X, kRange)
% Use
%   Implementation of k-means with hold-out cross-validation to determine
%   the best number of clusters to use within user-input range.
% Input
%   X : m-samples training set, where each row is a sample feature vector
%   kRange : range of k-values for cross-validation
% Output
%   c : cluster labels for each point
%   mu : cluster centroid positions, where each column is a centroid
%   distortionValue: distortion function value
%   k : best k-value out of kRange

    % constants
    TRAIN_PROP = 0.7;
    NRESTARTS = 30;
    NDATA = size(X, 1);
    
    % initialization
    c = -1 * ones(NDATA, 1);
    mu = inf * ones(size(X, 2), kRange(1));
    k = kRange(1);
    
    % randomly split X to train and test sets
    nTrain = floor(TRAIN_PROP * NDATA);
    perm = randperm(NDATA);
    Xperm = X(perm, :);
    XTrain = Xperm(1:nTrain);
    XTest = Xperm(nTrain + 1:end);
    
    % try all k-values
    for kNew = kRange
        [cNew, muNew, ~] = myKMeans(XTrain, kNew, NRESTARTS);
        if distortion(XTest, cNew, muNew) < distortion(XTest, c, mu)
            c = cNew;
            mu = muNew;
            distortionValue = distortion(X, c, mu);
            k = kNew;
        end % if
    end % for kNew

end


function distortion = distortion(X, c, mu)
% Use
%   Computes the distortion function value.
% Input
%   X : m-samples training set, where each row is a sample feature vector
%   c : cluster labels for each point
%   mu : cluster centroid positions, where each column is a centroid
% Output
%   distortion : distortion function value

    if c(1) < 0
        distortion = inf;
    else
        distortion = sum(norms(X' - mu(:, c), 2)) ^ 2;
    end % if

end


function [c, mu, distortionValue, k] = kfold(X, kRange)
% Use
%   Implementation of k-means with k-fold cross-validation to determine the
%   best number of clusters to use within user-input range.
% Input
%   X : m-samples training set, where each row is a sample feature vector
%   kRange : range of k-values for cross-validation
% Output
%   c : cluster labels for each point
%   mu : cluster centroid positions, where each column is a centroid
%   distortionValue: distortion function value
%   k : best k-value out of kRange

    % constants
    KFOLD = 10;
    NDATA = size(X, 1);
    
    % randomly split X into KFOLD disjoint subsets
    assignments = randi(KFOLD, NDATA, 1);
    
    for kNew = kRange
        for kfold = 1:KFOLD
            XTrain = X(, :)
        end % for kfold
    end % for kNew

end